# Local LLM Backend Configuration
# Choose which local LLM backend to use: ollama or vllm
# 
# Options:
#   ollama - Use Ollama for local LLM inference (port 11434)
#   vllm   - Use vLLM for local LLM inference (port 8000)
#
# Default: ollama
LOCAL_LLM_BACKEND=vllm

