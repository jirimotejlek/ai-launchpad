# vLLM Configuration with HuggingFace Token
# Copy this file to the project root as .env
# Example: cp env_templates/vllm.env .env

# HuggingFace Hub Token (required for gated models like google/gemma-2b)
# Get your token from: https://huggingface.co/settings/tokens
HUGGING_FACE_HUB_TOKEN=hf_your_token_here

# Override the default vLLM model
VLLM_MODEL=Qwen/Qwen3-4B-Instruct-2507-FP8

# Optional: GPU memory utilization (0.0-1.0, default 0.85)
# Reduce to 0.7 or 0.6 if you get "Free memory is less than desired" errors
# VLLM_GPU_MEMORY_UTILIZATION=0.75

# Note: .env is already in .gitignore and will not be committed to git

